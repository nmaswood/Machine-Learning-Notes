{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1\n",
    "\n",
    "## Question\n",
    "\n",
    "__Overfitting of polynomial matching__: We have shown that the predicto defined in __2.3__ leads to overfitting . While this predictor seems very unnatural, the goal of this exercise is to show that if can be described as a thresholded polynomial. That is. show that given a training set $$ S = \\{(x_i, f(x_i))^{m}_{i=1} \\subset (\\mathbb{R}^d \\times \\{0,1\\})^m$$ there exists a polynomial $p_s$ such that $h_s(x) = 1$ if and only if $p_S(x) \\geq 0$, where $H_s$ is as defined in __2.3__. It follows that the class of all thresholded polynomials using ERM may overfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Consider the following polynomial:\n",
    "\n",
    "$$f'(x) = - \\prod_{i=1}^m (x - x_iy_i)^{2} $$\n",
    "\n",
    "If $x$ is a training example and $f(x) = 1$ then  $f'(x) = 0$. $ 0 \\geq 0$ so it would be classified as $1$. Otherwise the output will always be negative. This will correctly classify examples of the form $(x_i, 0)$ and also output $0$ for examples that have not been seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## Question\n",
    "\n",
    "Let $\\mathcal{H}$ be a class of binary classifier overs a domain $\\mathcal{X}$. Let $\\mathcal{D}$ be an unknown distribution over $\\mathcal{X}$ and let $f$ be the target hypothesis in $\\mathcal{H}$. Fix some $h \\in \\mathcal{H}$. Show that the expected value of $L_S(h)$ over the choice $S_{|x}$ equals $L_{\\mathcal{D}, f}(h)$\n",
    "\n",
    "$$ \\mathbb{E}_{S|x \\sim \\mathcal{D}^m}[L_s(h)] = L_{\\mathcal{D}, f}(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer \n",
    "\n",
    "\\begin{align}\n",
    "L_{\\mathcal{D}, f}(h) &= \\mathbb{P}[h(x) \\neq f(x)] = \\mathcal{D}(\\{x : h(x) \\neq f(x)\\})\\\\\n",
    "L_{\\mathcal{D}, f}(h) &= \\frac{\\mathbb{1}[ h(x) \\neq f(x)]}{m}\\\\\n",
    "L_s(h) &= \\frac{|\\{i \\in [m]: h(x_i) \\neq y_i \\}|}{m}\\\\\n",
    "L_{\\mathcal{D}, f}(h) &= L_s(h)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "\n",
    "## Question\n",
    "\n",
    "__Axis aligned rectangle:__ An axis aligned rectangle classifier in the plane is a classifier that assigns the value 1 to a point $\\iff$ its inside a certain rectangle.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "h(a_1, b_1, a_2, b_2)(x_1, x_2)=\n",
    "\\begin{cases}\n",
    "  1, & \\text{if}\\ a_1 \\leq x_1 \\leq b_1 \\text{ and } a_2 \\leq x_2 \\leq b_2\\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The class of all axis aligned rectangles in the plane is defined as:\n",
    "\n",
    "\n",
    "$$\\mathcal{H}^2 = \\{h(a_1, b_1, a_2, b_2): a_1 \\leq b_1  \\text{ and } a_2 \\leq b_2  \\}$$\n",
    "\n",
    "*Assume R. Assumption*\n",
    "\n",
    "1. Let $A$ be the algorithim that returns smallest rectangle. Show that $A$ is an __ERM__\n",
    "2. Show that if $A$ recieves $m' \\geq \\frac{4\\ln(\\frac{4}{\\delta})}{\\epsilon}$ samples then it will have $(1-\\delta, \\epsilon)$ confidence and error.\n",
    "\n",
    "    * Show that $R(S) \\subseteq R^*$\n",
    "        * Suppose $R^* \\subset R(S)$. Then there would be some labeled example in $R(S)$ that was not in $R^*$. However, since $R^*$ is the rectangle that produces the labels this is a contradiction. Therefore $R(S)$ can be a smaller rectangle denoting that its seen too little data or it can be equal to  $R^*$ denoting that it has learned the distribution completely.\n",
    "    * Show that if $S$ contains positive examples in all rectangles $\\{R_1, \\ldots, R_4\\} $ then hypothesis $A$ has error at most $\\epsilon$\n",
    "    * For each $i \\in \\{1 \\ldots 4\\} $ upper the probability that $S$ does not contain an example from $R_i$\n",
    "        * Each of these rectangles is an rectanglar slice away from being equal to the correct predictor. Taking in sum each of these rectangles has $\\frac{\\epsilon}{4}$ error. For a single rectangle the probabilty of having no samples in that space is $1 - \\frac{\\epsilon}{4}$\n",
    "        * Therefore cumulative error over sample size $m$ is $(1 - \\frac{\\epsilon}{4})^m$\n",
    "    * Use union bound to complete argument  \n",
    "        * Because there are four rectangles we have $4 (1 - \\frac{\\epsilon}{4})^m$\n",
    "        * $$ m = \\frac{4 \\ln(\\frac{4}{\\delta})}{\\epsilon} $$\n",
    "3. Repeat previous question for class of aligned rectangles in $\\mathbb{R}^d$ \n",
    "4. Show that runtime of applying alg $A$ mentioned earlier is polynomial in $d, \\frac{1}{\\epsilon}, \\ln(\\frac{1}{\\delta})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
