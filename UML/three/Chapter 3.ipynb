{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Formal Learning Model\n",
    "\n",
    "## 1 Pac Learning \n",
    "\n",
    "__If the ERM wrt to that class is applied on a sufficiently large training sample then the output will be PAC__\n",
    "\n",
    "* Pac Learnability\n",
    "    * A hypothesis class $H$ is PAC learnable if $\\exists m_H :(0,1)^2 \\rightarrow \\mathbb{N}$  and a learning algorithim with the property: $\\forall \\epsilon, \\delta \\in (0,1)$ for every distribution $D \\text{ over } X$, and for every labeling function $f: X \\rightarrow {0,1}$, if the realizable assumption holds with respect to $H,D,F$ then when running the learning algorhtim on $ m \\geq m_H(\\epsilon, \\delta)$ i.i.d. examples generated by $D$ and labeled by $f$, the algorhtim returns a hypothesis $h$ such that, with probability of at least $1 - \\delta$ , $L_{(D,f)}(h) \\leq \\epsilon$\n",
    "    * $\\epsilon$ denotes how far classifier can stray from optimal\n",
    "        * forgives classifier for making minor errors\n",
    "        * There is always a chance data subset is uninformative\n",
    "    * Sample Complexity\n",
    "        * $m_H: (0,1) \\rightarrow \\mathbb{N}$ determines how many examples are required to generate a PAC solution. \n",
    "        * If $H$ is PAC learnable there are many functions $m_H$ that satisfy the requirements given\n",
    "        * $ \\epsilon, \\delta, m_H(\\epsilon, \\delta)$ is the minimal integer that satisifies requirements of PAC learning with accuracy and confidence $\\epsilon, \\delta$\n",
    "    \n",
    "## 2 More General Learning Model\n",
    "\n",
    "* Removing realzibility assumption\n",
    "    * Assumption is too strong so we waive it with the agnostic PAC learning model\n",
    "    * e.g. the model you have chosen cannot fit complexity of data\n",
    "    \n",
    "\n",
    "### 2.1 Releasing Realizibility Assumption\n",
    "\n",
    "* The realizability assumption requires that  $\\exists h^* \\in \\mathcal{H} \\text{ s.t. } \\mathbb{P}_{x \\sim \\mathcal{D}}[h^*(x) = f(x)] = 1$\n",
    "* A hypothesis exists which can completely learn the distribution;  Obviously, not always achievable\n",
    "\n",
    "#### Empirical and True Error Revised\n",
    "\n",
    "* __The true error (risk) of a prediction rule $h$ is __\n",
    "     $$ L_{\\mathcal{D}}(h) \\stackrel{\\text{def}}{=} \\mathbb{P}_{(x,y) \\sim \\mathcal{D}} [h(x) \\neq y] \\stackrel{\\text{def}}{=} \\mathcal{D}(\\{(x,y) : h(x) \\neq y\\})$$\n",
    "    * What is the probability of being wrong?\n",
    "    * The goal of learning is to find a $h$ that minimizes P.A. true risk.\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Optimal Predictor\n",
    "\n",
    "  \\begin{equation}\n",
    "    f_{\\mathcal{D}}(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ \\mathbb{P}[y = 1 | x] \\geq \\frac{1}{2} \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    "\n",
    "\n",
    "* No other classifier has a lower error.\n",
    "* However since we do not know $\\mathcal{D}$ we cannot ever use it\n",
    "* Thus we seek an algorithim whose error is not much larger than Bayes Optimal\n",
    "\n",
    "__Definition 3.3__\n",
    "\n",
    "* Agnostic PAC Learnibility\n",
    "    * A hypothesis class $\\mathcal{H}$ is agnostic PAC learnable with respecct to a set $Z$ and a loss function $ \\ell : \\mathcal{H} \\times Z \\rightarrow \\mathbb{R}_+$ if $\\exists m_{\\mathcal{H}}: (0,1)^2 \\rightarrow \\mathbb{N}$ and learning algorithim with the following property: For every $\\epsilon, \\delta \\in (0,1)$ and for every distribution $\\mathcal{D}$ over $Z$, when running the learning algorithim on $m \\geq m_{\\mathcal{H}}(\\epsilon, \\delta)$ examples generated by $\\mathcal{D}$, the algorithim returns $h \\in \\mathcal{H}$ such that, with probability of at least $1- \\delta$ $$L_{\\mathcal{D}}(h) \\leq \\min  L_{\\mathcal{D}}(h') + \\epsilon$$\n",
    "    *\n",
    "    *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pac Learner\n",
    "\n",
    "Basically PAC Learnable says that if you define some $\\epsilon, \\delta$  which take values over the range $(0,1)$ then there must exist some function $f(\\epsilon, \\delta)$ that returns a $\\mathbb{N}$ of the training examples needed to achieve a predictor with loss less than $\\epsilon$ with a probability of $1- \\delta$\n",
    "\n",
    "\n",
    "### Agnostic Pac Learner\n",
    "\n",
    "A hypothesis class $H$ is PAC learnable if $\\exists m_H :(0,1)^2 \\rightarrow \\mathbb{N}$  and a learning algorithim with the property: $\\forall \\epsilon, \\delta \\in (0,1)$ for every distribution $D \\text{ over } X$, and for every labeling function $f: X \\rightarrow {0,1}$, if the realizable assumption holds with respect to $H,D,F$ then when running the learning algorhtim on $ m \\geq m_H(\\epsilon, \\delta)$ i.i.d. examples generated by $D$ and labeled by $f$, the algorhtim returns a hypothesis $h$ such that, with probability of at least $1 - \\delta$\n",
    "\n",
    "$$ L_{D}(h) \\leq \\min_{h' \\in \\mathcal{H}} L_{\\mathcal{d}}(h')  + \\epsilon$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
