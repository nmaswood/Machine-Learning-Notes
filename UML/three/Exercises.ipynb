{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1\n",
    "\n",
    "## Question \n",
    "\n",
    "__Monotonicity of sample complexity__ Let $\\mathcal{H}$ be a h class for a binary classification task. Suppose that $H$ is PAC Learnable by $m_{\\mathcal{H}}(.,.)$ Show that  $m_{\\mathcal{H}}(.,.)$ is monotonically nonincreasing for each of its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## Question\n",
    "\n",
    "Let $\\mathcal{X}$ be the discrete domain, and let $\\mathcal{H}_{singleton} = \\{h_z : z \\in \\mathcal{X} \\;\\cup \\{h^-\\}$  where $\\forall \\; z \\in \\mathcal{X}, h_z$ is the function defined by\n",
    "\n",
    "\\begin{equation}\n",
    "    h_z(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x = z \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    " \n",
    "$h^-$ is simply the all negative hypothesis\n",
    "1. Describe an algorithim that implements the ERM rule for learning $\\mathcal{H}_{singleton}$ in the realizable setup.\n",
    "2. Show that $\\mathcal{H}_{singleton}$ is PAC Learnable. Provide an upper bound on sample complexity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "## Question\n",
    "\n",
    "Let $\\mathcal{X} = \\mathbb{R}^2, \\mathcal{Y} = \\{0,1\\}$ and let $\\mathcal{H}$ be the class of concentric circles in the plane, that is $\\mathcal{H} = \\{h_r : r \\in \\mathbb{R}_+\\}$, where $h_r(x) = 1_{[||x|| \\leq r]}$\n",
    "\n",
    "Prove that $\\mathcal{H}$ is PAC Learnable and is bounded by \n",
    "\n",
    "$$ m \\leq \\frac{\\ln\\frac{1}{\\delta}}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "\n",
    "Prove that the hypothesis class of conjunctions over $d$ variables is PAC learnable and bound its sample complexity.  Propose an algorithim that implements the ERM Rule whose run time is $\\;O(dm)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "\n",
    "Let $\\mathcal{X}$ be a domain and let $ S = \\{D_i\\}\\;, |S| = m$ Let $\\mathcal{H}$ be a set of finite classifiers over $\\mathcal{X}$ and let $f \\in \\mathcal{H}$ Suppose we are getting a sample $S$ of $m$ examples, such that the instances are __i.__ but not __i.d.__. The $i$th example is is sampled fmor $\\mathcal{D}_i$ and then $y_i$ is set to be $f(x_i)$. Let $\\overset{\\_}{\\mathcal{D}}$ denote the average.\n",
    "\n",
    "Fix $\\epsilon \\in \\{0,1\\}$\n",
    "\n",
    "Prove\n",
    "\n",
    "$$ \\mathbb{P}[ \\exists \\; h \\in \\; \\mathcal{H} \\text{ s.t. } L_{(\\mathcal{D}, f)} > \\epsilon \\; \\land  L_{(s,f)}(h) = 0] \\leq \\; |\\mathcal{H}| e ^{-\\epsilon m }$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
