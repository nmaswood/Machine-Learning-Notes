{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A Formal Model\n",
    "\n",
    "* Domain Set\n",
    "    * Arbitrary set $X$ which is the set of objects we wish to label\n",
    "* Label Set\n",
    "    * The set  $Y = \\{0,1\\}$\n",
    "* Training Data\n",
    "    * $S = ((x_1, y_1) \\ldots (x_n,y_n))$ is a finite sequence of pairs in $ X \\times Y$\n",
    "* Learner's Output\n",
    "    * A prediction rule $h: X \\rightarrow  Y$\n",
    "    * Employed to map unseen points to label set\n",
    "* Simple Data Generation Model\n",
    "    * $D$\n",
    "    * Assume that data is generated from unknown distribution\n",
    "    * There exists some correct labeling function $f$ which we do not have access to \n",
    "* Measure of Success\n",
    "    * The error of $h$ is the probability to draw a random instance $x$ from $D$ s.t. $h(x) \\neq f(x)$\n",
    "    * The error of a prediction rule is\n",
    "    * $$ L_{D,f}(h) = \\mathbb{P}_{x ~ D}[h(x) \\neq f(x)] = D(\\{x : h(x) \\neq f(x)\\}$$\n",
    "    * The error of such $h$ is the probability of randomly choosing an example x for which $h(x) \\neq f(x)$\n",
    "    * $L$ which stands for loss is typically used for error\n",
    "* Note\n",
    "    * The learner is blind to underlying distribution\n",
    "    * Can only interact with environment through observed data drawn from distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Empirical Risk Minimization\n",
    "\n",
    "* A learning algorithim recieves a training set $S$ sampled from $D$ and outputs a predictor $h : X \\rightarrow Y$\n",
    "* The goal of learning is to find a $h_s$ which minimizes errror w.r.t. $D, f$\n",
    "* coming up with a predictor $h$ that minimizes $L_{S(h)}$is called ERM\n",
    "\n",
    "## 1. Overfitting\n",
    "\n",
    "* ERM seems like a very natural approach but it can not always work.\n",
    "* Pathological\n",
    "    * Consider the function that returns the label of data it has already seen and returns 0 on  data it has not seen \n",
    "    * This hypothesis will have zero 0 ERM and will be no better than chance on unseen data.\n",
    "* Hypothesis  has very good training ERM, but very poor results on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inductive Bias \n",
    "\n",
    "\n",
    "* Search for conditions where ERM does not overfit\n",
    "* Apply learning rule over a restricted hypothesis class\n",
    "* Choice of restriction should ideally be based on prior knowledge\n",
    "\n",
    "## Finite Hypothesis Class\n",
    "\n",
    "* The simplest bound is imposing an upper bound on the number of predictors in $H$.\n",
    "\n",
    "* The Realizability Assumption\n",
    "    * There exists $h^* \\in H$ s.t. $L_{D,f} (h^*) = 0$\n",
    "* We are interested in true risk and not empirical risk\n",
    "* The IID Assumption\n",
    "    * Data is sampled independently and identically distributed  according to $D$ and then labeled by $f$\n",
    "* It is possible that we get a non-representative sample of data\n",
    "    * This probability is denoted by $\\delta$\n",
    "    * $(1- \\delta)$ is our confidence parameter\n",
    "    \n",
    "* The larger your dataset the smaller the probability of having dataset which is misleading or non-characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
