{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Learning Algorithim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discriminative Algorithims__\n",
    "\n",
    "These types of algorithims model $ p(y|x, \\theta)$. For example logistic regression is a discriminative model where the algorithim tries to find a line of best fit that seperates the data.\n",
    "\n",
    "\n",
    "__Generative Algorithims__\n",
    "\n",
    "Generative algorithims instead try o model $p(x|y)$ and $p(y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One uses __Baye's Rule__ to model $p(y|x)$\n",
    "\n",
    "$$ p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "And then finds\n",
    "\n",
    "$$ \\text{argmax } p(y|x) = \\text{argmax } \\frac{p(x|y)p(y)}{p(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Discriminant analysis\n",
    "\n",
    "\n",
    "Assume that $p(x|y)$ is distributed according to $\\mathcal{N}(\\mu, \\Sigma)$\n",
    "\n",
    "### Example\n",
    "\n",
    "Let us say we have a classifcation problem with $x \\in \\mathbb{R}^d$ and $ y \\in \\{0,1\\}$\n",
    "\n",
    "The model is \n",
    "\\begin{align}\n",
    "y &\\sim \\text{Ber}(\\phi) \\\\\n",
    "x| y = 0 &\\sim \\mathcal{N}(\\mu_0, \\Sigma) \\\\\n",
    "x| y = 1 &\\sim \\mathcal{N}(\\mu_1, \\Sigma) \\\\\n",
    "\\end{align}\n",
    "\n",
    "The log liklihood is\n",
    "\\begin{align}\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) &= \\log \\prod_{i=1}^{m} p(x^i, y^i; \\phi, \\mu_0, \\mu_1, \\Sigma)\\\\\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) &= \\log \\prod_{i=1}^{m} p(x^i, y^i; \\mu_0, \\mu_1, \\Sigma) p(y^i; \\phi)\n",
    "\\end{align}\n",
    "\n",
    "Solving for all parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\phi &= \\frac{1}{m} \\sum_{i=1}^m 1[y_i =1] \\\\\n",
    "\\mu_0 &= \\frac{\\sum_{i=1}^m 1[y^i = 0] x_i}{\\sum_{i=1}^m 1[y^i = 0]} \\\\\n",
    "\\mu_1 &= \\frac{\\sum_{i=1}^m 1[y^i = 1] x_i}{\\sum_{i=1}^m 1[y^i = 1]} \\\\\n",
    "\\Sigma &=  \\frac{1}{m} \\sum_{i=1}^m (x^i - \\mu_{y^i})(x^i - \\mu_{y^i})^T\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one views $p(y=1 | x, \\phi, \\mu_0, \\mu_1, \\Sigma)$ as a function of $x$ it can be expresed in the form \n",
    "$$ p(y = 1 |x, \\phi, \\Sigma, \\mu_0, \\mu_1) \\frac{1}{1 + exp(-\\theta^T\n",
    "x})$$\n",
    "\n",
    "\n",
    "### Model Comparisons\n",
    "\n",
    "\n",
    "GDA makes stronger assumptions (that the data is normally distributed) If your assumption is correct, then your model is better off. If you are wrong then you're probably worse off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "\n",
    "### Spam Classification\n",
    "\n",
    "\n",
    "Represent the words present in a email as a vector\n",
    "$$ x \\in \\mathbb{R}^d, d = \\text{ number of unique words in corpus}$$ \n",
    "\n",
    "We want to model $p(x|y)$ however we have too many features\n",
    "\n",
    "$$ x = \\{0,1\\}^D$$\n",
    "\n",
    "Therefore we make the simplifying assumption that the features are condiitionally independent.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}) &= \\prod_{i=1}^m p(x^i, y^i)\\\\\n",
    "\\phi_{j|y=0} &= \\frac{1[y^i =1 \\land x^i =0]}{\\sum_{i=1}^m 1[y^i = 0]} \\\\\n",
    "\\phi_{j|y=1} &= \\frac{1[y^i =1 \\land x^i = 1]}{\\sum_{i=1}^m 1[y^i = 1]} \\\\\n",
    "\\phi_{y} &= \\frac{\\sum_{i=1}^m 1[y^i =1]}{m}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "To make a prediction\n",
    "\n",
    "$$ p(y = 1 | x) = \\frac{p(x|y=1)p(y=1)}{p(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Smoothing\n",
    "\n",
    "\n",
    "Words that are not been seen need to have something done so that probility is not $0/0$\n",
    "\n",
    "\n",
    "$$ \\theta_j = \\frac{\\sum_{i=1}^m 1[z^i = j] + 1}{m + k}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event models for classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
