{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic  Regresion\n",
    "\n",
    "Let us define a problem where we would like to map\n",
    "\n",
    "$$f: x \\rightarrow y | x \\in \\mathbb{R}^d   , y \\in \\{0,1\\}$$\n",
    "\n",
    "Traditional linear regression is not a very smart choice so let us redefine $h_{\\theta}(x)$\n",
    "\n",
    "\\begin{align}\n",
    "h_{\\theta}(x) &= g(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}\\\\\n",
    "g(z) &= \\frac{1}{1 + e^-z}\n",
    "\\end{align}\n",
    "\n",
    "* $g(z)$ called the sigmoid function is chosen because it has some nice properties.\n",
    "    * The range of the function maps between $(0,1)$ this always us to intrepret it probabilistically.\n",
    "    * Easy to calculate derivative\n",
    "        * $g'(z) = g(z) (1 - g(z))$\n",
    "    * Mathetmically works (according to Ng)\n",
    "![alt text](assets/sig.png \"Title\")\n",
    "\n",
    "\n",
    "We can find the optimal theta as follows\n",
    "\n",
    "__Find the deravative of MLE__\n",
    "\\begin{align}\n",
    "P(y &= 1 | x, \\theta) = h_{\\theta}(x)\\\\\n",
    "P(y &= 0 | x, \\theta) = 1 - h_{\\theta}(x)\\\\\n",
    "\\ell(\\theta) &= \\sum_{i=1}^m y^i \\ln(h(x^i)) + (1 -y ^i) \\ln(1 - h(x^i))\\\\\n",
    "\\frac{\\partial}{\\partial \\theta_j} &= (y - h_\\theta(x))x_j\n",
    "\\end{align}\n",
    "\n",
    "__Perform Gradient Descent__\n",
    "\n",
    "$$ \\theta_j := \\theta_j + \\alpha(y^i - h_\\theta(x^i)) x_j$$\n",
    "\n",
    "\n",
    "__Evaluation__\n",
    "\\begin{equation}\n",
    "f_{\\mathcal{D}}(x)=\n",
    "\\begin{cases}\n",
    "  1, & \\text{if}\\ h(x = 1) \\geq \\frac{1}{2} \\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Perceptron\n",
    "\n",
    "* Attempts to find dividing line between linearly seperable data\n",
    "* Online Algorithim\n",
    "* Not interpretable\n",
    "\n",
    "\\begin{equation}\n",
    "g(z) =\n",
    "\\begin{cases}\n",
    "  1, & \\text{if}\\ z \\geq 0\\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "__Update Rule__\n",
    "\n",
    "$$\\theta_j := \\theta_j + \\alpha ( y^i - h_{\\theta}(x^i))x_j$$\n",
    "\n",
    "![alt text](assets/perceptron.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method\n",
    "\n",
    "* Approximate $f$ via linear functoin that is tanget to f at current guess.\n",
    "* 2nd Order Method\n",
    "* Convergences faster than gradient descent however it requires storing $H$ which is expensive when one has large # of parameters\n",
    "\n",
    "$$\\theta := \\theta - \\frac{f(\\theta)}{f'(\\theta)}$$\n",
    "\n",
    "Generalization of approach\n",
    "\\begin{align}\n",
    "\\theta &:= \\theta - H^{-1}\\nabla_\\theta \\ell(\\theta)\\\\\n",
    "H_{ij} &= \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_i \\theta_j}\n",
    "\\end{align}\n",
    "\n",
    "![alt text](assets/newton.gif \"Title\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models\n",
    "\n",
    "\n",
    "Let the expotential family of distributions be those of the form \n",
    "$$ p(y |\\eta) = b(y)exp(\\eta^T T(y) - a(\\eta)$$\n",
    "\n",
    "* $\\eta$ is called the __natural parameter__ or the __canonical parameter__\n",
    "* $T(y)$ is the __sufficient statistic__\n",
    "    * usually $T(y) = y$\n",
    "* $a(\\eta)$ the partition function\n",
    "    * Normalization constant\n",
    "\n",
    "\n",
    "Many common models can are GLMS such as the __Bernoulli Distribution__ and the __Normal Distribution__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructiong GLMS\n",
    "\n",
    "To derive a GLM for a problem make the following assumptions \n",
    "\n",
    "1. $$y | x, \\theta \\sim \\text{ Expo. Family} (\\eta)$$\n",
    "2. Our goal is to predict $T(y) | x$. We want our $h(x)$ output to satisfy $h(x) = E[y|x]$\n",
    "3. $\\eta$ and $x$ are related linearly $$ \\eta = \\theta^T x$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
